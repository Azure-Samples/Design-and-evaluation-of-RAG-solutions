## 5.3. Testing end-to-end answer quality

The purpose of these tests is to evaluate the quality of the responses based on a reference dataset containing pairs of questions and answers. Is it important to have a high-quality dataset for this purpose, that is representative of the different questions that will be asked in a real-life situation, along with realistic expected answers from final users.

It is important to gather a representative test set with questions and the expected answers validated from the client’s business area.

As was described in 5.2, the automatic generation of synthetic questions and answers pairs from documents reviewed by the business area would be very useful to get a more representative test set of questions with expected answers.

Then we can conduct tests on the quality of answers by comparing the meaning of the expected answers with the answers generated by the solution, using in the answer generation the chunks filtered with more than 90% confidence by the re-ranker.

The evaluation can be done with the following GPT-4 Turbo prompts:

**System prompt:** "You are an AI assistant that helps people validate the accuracy and completeness of a response against a ground trust. Given the user's question, the expected ground truth answer and the current answer generated by a RAG pattern (everything in Spanish), compare the meaning of both answers and assess if the current answer addresses the user's question and select a number that best describes this assessment considering the following guidelines:

0: The generated answer and the expected answer have completely different meanings, and the generated answer does not address the user's question.

1: The generated answer is very similar in meaning to the expected answer but lacks some crucial information, and it partially addresses the user's question.

2: The generated answer is well-aligned with the expected answer, capturing the main points accurately, and fully addressing the user's question.

3: The generated answer not only aligns with the expected ground truth and answers the user's question but also adds valuable additional details or insights.

Based on these guidelines, provide only the number that best represents the relationship between the generated answer and the expected ground truth answer. Do not include any explanation, only the number.”

**User prompt:** “Question: \[question\]. Expected Ground Truth Answer: \[correct answer\]. Generated Answer: \[answer\]. Your evaluation:”

The test has to consider every chunking and searching options described in 5.1, and the results can be presented in a table as the following:

Total number of answers generated semantically similar than expected answers: 
| Chunking technique | Search upper/lowercase | Embedding Model | Embedding fields |
| --- | --- | --- | --- |
| Langchain semantic chunking | 1 vector field with Content |     |     |
|  | 1 vector field with Title and Content |     |     |
|  | 2 vector fields with title and content |     |     |
|  | 3 vector fields with title, content and Others |     |     |
| GPT-4T semantic chunking | 1 vector field with Content |     |     |     |
|  | 1 vector field with Title and Content |     |     |
|  | 2 vector fields with title and content |     |     |
|  | 3 vector fields with title, content and Others |     |     |     |     |
| Langchain & tiktoken max. 512 tokens with 25% overlapping | 1 vector field with Content |     |     |
|  | 1 vector field with Title and Content |     |     |
|  | 2 vector fields with title and content |     |     |
|  | 3 vector fields with title, content and Others |     |     |
| Langchain & tiktoken max. 1024 tokens with 25% overlapping | 1 vector field with Content |     |     |
|  | 1 vector field with Title and Content |     |     |
|  | 2 vector fields with title and content |     |     |
|  | 3 vector fields with title, content and Others |     |     |

It is important to highlight that the insights from these tests compared with the previous search tests could be that **although the search results and answers may not include the expected document ID**, **the generated answers could be of higher quality compared to the expected answers in more cases**. The reason of this potential result is the documents that can be valid to answer the question can be duplicated or have similar content.

**Code Snippet:**
[evaluate_answer.ipynb](./evaluate_answer.ipynb)