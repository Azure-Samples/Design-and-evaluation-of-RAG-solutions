{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end process\n",
    "\n",
    "This code demonstrate the complete process with the following tasks:\n",
    "1. Convert HTML files to markdown format\n",
    "2. Chunk markdown content with the maximum number of tokens specified (in this example, 512 tokens with 25% of overlapping)\n",
    "3. Create the index and upload the chunks (in this example, the embeddings are created with the model ada-02)\n",
    "4. Test the search and answer generation creating the Excel files with the results of answers evaluation\n",
    "\n",
    "## Prerequisites\n",
    "+ An Azure subscription, with [access to Azure OpenAI](https://aka.ms/oai/access).\n",
    "+ A Document Intelligence service with its end-point and API key.\n",
    "+ An Azure OpenAI service with the service name and an API key.\n",
    "+ A deployment of the text-embedding-ada-002 embedding model on the Azure OpenAI Service.\n",
    "+ An Azure AI Search service with the end-point, API Key and the index name to create.\n",
    "\n",
    "We used Python 3.12.5, [Visual Studio Code with the Python extension](https://code.visualstudio.com/docs/python/python-tutorial), and the [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) to test this example.\n",
    "\n",
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install azure-search-documents\n",
    "!pip install nbimporter\n",
    "!pip install nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and create AOAI and AI Search clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.search.documents import SearchClient\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../..')\n",
    "from rag_utils import load_files, get_markdown_with_doc_intel, chunk_with_max_tokens, create_index, index_documents, generate_answers_and_questions, execute_test, generate_chunks_with_aoai, generate_topics_and_documents\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# DOCUMENT INTELLIGENCE\n",
    "doc_intel_endpoint = os.getenv(\"DOC_INTEL_ENDPOINT\")\n",
    "doc_intel_key = os.getenv(\"DOC_INTEL_KEY\")\n",
    "doc_intel_client = DocumentIntelligenceClient(endpoint=doc_intel_endpoint, credential=AzureKeyCredential(doc_intel_key))\n",
    "\n",
    "# AZURE AI SEARCH\n",
    "ai_search_endpoint = os.environ[\"SEARCH_SERVICE_ENDPOINT\"]\n",
    "ai_search_apikey = os.environ[\"SEARCH_SERVICE_QUERY_KEY\"]\n",
    "ai_search_index_name = os.environ[\"SEARCH_INDEX_NAME\"]\n",
    "ai_search_credential = AzureKeyCredential(ai_search_apikey)\n",
    "\n",
    "# CREATE AZURE AI SEARCH CLIENT\n",
    "ai_search_client = SearchClient(endpoint=ai_search_endpoint, index_name=ai_search_index_name, credential=ai_search_credential)\n",
    "\n",
    "aoai_api_version = '2024-02-15-preview'\n",
    "\n",
    "# AOAI FOR ANSWER GENERATION\n",
    "aoai_answer_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "aoai_answer_apikey = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "aoai_answer_model_name = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "# Create AOAI client for answer generation\n",
    "aoai_answer_client = AzureOpenAI(\n",
    "    azure_deployment=aoai_answer_model_name,\n",
    "    api_version=aoai_api_version,\n",
    "    azure_endpoint=aoai_answer_endpoint,\n",
    "    api_key=aoai_answer_apikey\n",
    ")\n",
    "\n",
    "# AZURE OPENAI FOR RERANKING\n",
    "aoai_rerank_endpoint = os.environ[\"AZURE_OPENAI_RERANK_ENDPOINT\"]\n",
    "azure_openai_rerank_key = os.environ[\"AZURE_OPENAI_RERANK_API_KEY\"]\n",
    "rerank_model_name = os.environ[\"AZURE_OPENAI_RERANK_DEPLOYMENT_NAME\"]\n",
    "# Create AOAI client for reranking\n",
    "aoai_rerank_client = AzureOpenAI(\n",
    "    azure_deployment=rerank_model_name,\n",
    "    api_version=aoai_api_version,\n",
    "    azure_endpoint=aoai_rerank_endpoint,\n",
    "    api_key=azure_openai_rerank_key\n",
    ")\n",
    "\n",
    "# AZURE OPENAI FOR EMBEDDING\n",
    "aoai_embedding_endpoint = os.environ[\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"]\n",
    "azure_openai_embedding_key = os.environ[\"AZURE_OPENAI_EMBEDDING_API_KEY\"]\n",
    "embedding_model_name_ada = os.environ[\"AZURE_OPENAI_EMBEDDING_NAME_ADA\"]\n",
    "embedding_model_name_large_3 = os.environ[\"AZURE_OPENAI_EMBEDDING_NAME_LARGE_3\"]\n",
    "# Create AOAI client for embedding creation (ADA)\n",
    "aoai_embedding_client_ada = AzureOpenAI(\n",
    "    azure_deployment=embedding_model_name_ada,\n",
    "    api_version=aoai_api_version,\n",
    "    azure_endpoint=aoai_embedding_endpoint,\n",
    "    api_key=azure_openai_embedding_key\n",
    ")\n",
    "# Create AOAI client for embedding creation (LARGE-3)\n",
    "aoai_embedding_client_large_3 = AzureOpenAI(\n",
    "    azure_deployment=embedding_model_name_large_3,\n",
    "    api_version=aoai_api_version,\n",
    "    azure_endpoint=aoai_embedding_endpoint,\n",
    "    api_key=azure_openai_embedding_key\n",
    ")\n",
    "\n",
    "# Prepare the tests\n",
    "TESTS = {\n",
    "        # Test-name: Embeddings_fields | uppercase/lowercase) | embbeding_model | index_name | max_retrieve | max_generate\n",
    "        \"title_content_ada_512_search_upper_20_10\": (\"embeddingTitle, embeddingContent\", \"upper\", \"ada\", aoai_embedding_client_ada, \"project_assurance_ada_512\", 20, 10),\n",
    "        \"title_content_ada_512_search_upper_20_20\": (\"embeddingTitle, embeddingContent\", \"upper\", \"ada\", aoai_embedding_client_ada, \"project_assurance_ada_512\", 20, 20),\n",
    "        \"title_content_ada_512_search_lower_20_10\": (\"embeddingTitle, embeddingContent\", \"lower\", \"ada\", aoai_embedding_client_ada, \"project_assurance_ada_512\", 20, 10),\n",
    "        \"title_content_ada_512_search_lower_20_20\": (\"embeddingTitle, embeddingContent\", \"lower\", \"ada\", aoai_embedding_client_ada, \"project_assurance_ada_512\", 20, 20)\n",
    "        \n",
    "        # \"title_content_large_3_512_search_upper_20_10\": (\"embeddingTitle, embeddingContent\", \"upper\", \"large-3\", aoai_embedding_client_large_3, \"project_assurance_large_3_512\", 20, 10),\n",
    "        # \"title_content_large_3_512_search_upper_20_20\": (\"embeddingTitle, embeddingContent\", \"upper\", \"large-3\", aoai_embedding_client_large_3, \"project_assurance_large_3_512\", 20, 20),\n",
    "        # \"title_content_large_3_512_search_lower_20_10\": (\"embeddingTitle, embeddingContent\", \"lower\", \"large-3\", aoai_embedding_client_large_3, \"project_assurance_large_3_512\", 20, 10),\n",
    "        # \"title_content_large_3_512_search_lower_20_20\": (\"embeddingTitle, embeddingContent\", \"lower\", \"large-3\", aoai_embedding_client_large_3, \"project_assurance_large_3_512\", 20, 20),\n",
    "}\n",
    "\n",
    "# CONSTANTS\n",
    "MAX_CHUNK_TOKEN_SIZE = 512 # Max number of tokens for chunking\n",
    "CHUNK_OVERLAPPING=0.25 # 25% of overlapping between chunks\n",
    "SELECT_FIELDS=[\"id\", \"title\", \"content\"] # Fields to retrieve in the search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every HTML file in the input directory generate synthetic html content, convert them to markdown, generate synthetic Q&A pairs, chunk markdown documents, index them is AI Search, execute the test and evaluation of generated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Generate synthetic content about a customer's sector. Example: Telco\n",
    "input_dir=\"../output_telco\"\n",
    "generate_topics_and_documents(aoai_answer_client, aoai_answer_model_name, \"QuickConnect\", \"telecommunications\", input_dir)\n",
    "\n",
    "#input_dir = '..\\data_in' # Comment STEP 1 and uncomment this line if you want to use the html generated automatically about a Telco company that comes with that folder\n",
    "html_files = load_files(input_dir, '.html')\n",
    "\n",
    "all_chunks = []\n",
    "qa_data = {'question': [], 'answer': []}\n",
    "# Read the html files\n",
    "for i, html_file in enumerate(html_files):\n",
    "    print(f\"[{i + 1}]: {html_file['title']}\")\n",
    "    # print(f\"\\t[{html_file['content']}]\")\n",
    "\n",
    "    # STEP 2: Convert the html files to markdown format\n",
    "    print(f'\\tConverting to markdown...')\n",
    "    markdown = get_markdown_with_doc_intel(doc_intel_client, html_file['content'])\n",
    "    # print(f'markdown: [{markdown}]')\n",
    "    title = html_file['title'].replace('.html', '')\n",
    "\n",
    "    # STEP 3: Generate questions and answers pairs from the markdown content and prepare them to be salved in an Excel file\n",
    "    qa_pairs = generate_answers_and_questions(aoai_answer_client, aoai_answer_model_name, title + '. ' + markdown)\n",
    "    for qa in qa_pairs:\n",
    "        qa_data['question'].append(qa['question'])\n",
    "        qa_data['answer'].append(qa['answer'])\n",
    "\n",
    "    # STEP 4: Chunk the markdown content with a maximum number of tokens and a percentage of overlapping\n",
    "    # In this example, 512 tokens with 25% of overlapping\n",
    "    chunks = chunk_with_max_tokens(markdown, max_tokens=MAX_CHUNK_TOKEN_SIZE, overlap=CHUNK_OVERLAPPING)\n",
    "\n",
    "    # Chunk the markdown content with Semantic Chunking with GPT-4o with a maximum of tokens\n",
    "    # To test this chunking method comment the previous line and uncomment the following one:\n",
    "    # chunks = generate_chunks_with_aoai(aoai_answer_client, aoai_answer_model_name, markdown, MAX_CHUNK_TOKEN_SIZE)\n",
    "\n",
    "    # Prepare the list of chunks to be indexed\n",
    "    for chunk in chunks:\n",
    "        new_row = {\n",
    "            \"title\": title, \n",
    "            \"content\": chunk\n",
    "        }\n",
    "        all_chunks.append(new_row)\n",
    "\n",
    "    # Stop after 10 files do finish faster the test\n",
    "    if i + 1 == 10: break\n",
    "\n",
    "# Save questions and answers pairs in an Excel file\n",
    "df = pd.DataFrame(qa_data)\n",
    "qa_output_file = 'qa_pairs.xlsx'\n",
    "df.to_excel(qa_output_file, index=False)\n",
    "print(f'File {qa_output_file} saved')\n",
    "\n",
    "# STEP 5: Create the index for embeddings with ada-02\n",
    "index_name = 'project_assurance_ada'\n",
    "create_index(ai_search_endpoint, ai_search_credential, index_name, embedding_model_name_ada)\n",
    "\n",
    "# STEP 6: Index the chunks\n",
    "# In this example, the embeddings are created with the model ada-02\n",
    "index_documents(ai_search_endpoint, ai_search_credential, index_name, aoai_embedding_client_ada, embedding_model_name_ada, all_chunks)\n",
    "\n",
    "# STEP 7: Execute the tests with ada-02\n",
    "for test_name, (embedding_fields, case, embedding_model, embedding_client, index_name, max_retrieve, max_generate) in TESTS.items():\n",
    "    execute_test(ai_search_endpoint, ai_search_credential, SELECT_FIELDS, aoai_rerank_client, rerank_model_name, aoai_answer_client, aoai_answer_model_name, test_name, embedding_fields, case, embedding_model, embedding_client, index_name, max_retrieve, max_generate, qa_output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
